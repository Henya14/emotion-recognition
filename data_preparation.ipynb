{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79657d6c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b40b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from keras import layers\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0707f",
   "metadata": {},
   "source": [
    "# Declaring constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05a4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "MAX_SEQUENCE_LEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a0f1b",
   "metadata": {},
   "source": [
    "# Processing the video files\n",
    "Code from: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/video_transformers.ipynb#scrollTo=qidBV4ha1T1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad26eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_crop = layers.Cropping2D(cropping=(10, 70))\n",
    "\n",
    "def crop_image(frame):\n",
    "    cropped = image_crop(frame[None, ...])\n",
    "    cropped = cropped.numpy().squeeze()\n",
    "    return cropped\n",
    "\n",
    "def preprocess_video(path):\n",
    "    print(\"Start\", path)\n",
    "    try: \n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frame_cnt = 0\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Cut first ~1.3 seconds of the video\n",
    "            if frame_cnt < 40:\n",
    "                frame_cnt += 1\n",
    "                continue\n",
    "            \n",
    "            frame = tf.image.resize(frame, (240,368))\n",
    "            frame = crop_image(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frame = tf.image.resize(frame, (IMG_SIZE, IMG_SIZE)).numpy().astype(int)\n",
    "            frames.append(frame)\n",
    "    finally:\n",
    "        cap.release()\n",
    "    frames = np.array(frames)\n",
    "    mask = np.zeros((MAX_SEQUENCE_LEN,))\n",
    "    mask[:len(frames)] = 1\n",
    "    if len(frames) > MAX_SEQUENCE_LEN:\n",
    "        difference = len(frames) - MAX_SEQUENCE_LEN\n",
    "        frames = frames[int(np.ceil(difference/3)):-int(np.floor(2*difference/3)), :, :,:]\n",
    "    if len(frames) < MAX_SEQUENCE_LEN:\n",
    "        frames = np.pad(frames,pad_width=((0,MAX_SEQUENCE_LEN-len(frames)), (0,0), (0,0), (0,0)), mode=\"constant\")\n",
    "    \n",
    "    frames = np.transpose(frames, [3,0,1,2])\n",
    "    return frames, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f5b2a",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0ac8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\",\"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05615cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_prefix = os.path.join(\"data\", \"VideoFaceEmotion\", \"train\")\n",
    "emotion_dirs = os.listdir(train_folder_prefix)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for emotion_dir in emotion_dirs:\n",
    "    videos = os.listdir(os.path.join(train_folder_prefix, emotion_dir))\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df[\"video_name\"] = videos\n",
    "    temp_df[\"emotion\"] = emotion_dir.lower()\n",
    "    temp_df[\"dataset\"] = \"train\"\n",
    "    temp_df[\"video_path\"] = os.path.join(train_folder_prefix,emotion_dir,\"\") + temp_df[\"video_name\"]\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "#df.to_csv(os.path.join(\"data\",\"train.csv\"),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a370f10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               video_name   emotion     dataset  \\\n",
      "0       trainAnger001.avi     anger       train   \n",
      "1       trainAnger002.avi     anger  validation   \n",
      "2       trainAnger003.avi     anger  validation   \n",
      "3       trainAnger004.avi     anger       train   \n",
      "4       trainAnger005.avi     anger       train   \n",
      "..                    ...       ...         ...   \n",
      "420  trainSurprise067.avi  surprise       train   \n",
      "421  trainSurprise068.avi  surprise       train   \n",
      "422  trainSurprise069.avi  surprise  validation   \n",
      "423  trainSurprise070.avi  surprise  validation   \n",
      "424  trainSurprise071.avi  surprise       train   \n",
      "\n",
      "                                            video_path  \n",
      "0    data\\VideoFaceEmotion\\train\\Anger\\trainAnger00...  \n",
      "1    data\\VideoFaceEmotion\\train\\Anger\\trainAnger00...  \n",
      "2    data\\VideoFaceEmotion\\train\\Anger\\trainAnger00...  \n",
      "3    data\\VideoFaceEmotion\\train\\Anger\\trainAnger00...  \n",
      "4    data\\VideoFaceEmotion\\train\\Anger\\trainAnger00...  \n",
      "..                                                 ...  \n",
      "420  data\\VideoFaceEmotion\\train\\Surprise\\trainSurp...  \n",
      "421  data\\VideoFaceEmotion\\train\\Surprise\\trainSurp...  \n",
      "422  data\\VideoFaceEmotion\\train\\Surprise\\trainSurp...  \n",
      "423  data\\VideoFaceEmotion\\train\\Surprise\\trainSurp...  \n",
      "424  data\\VideoFaceEmotion\\train\\Surprise\\trainSurp...  \n",
      "\n",
      "[425 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"train_validation.csv\"))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e016c",
   "metadata": {},
   "source": [
    "## Creating a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9c699aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[df[\"emotion\"] == \"anger\"].sample(frac=0.2).index,\"dataset\"] = \"validation\"\n",
    "df.loc[df[df[\"emotion\"] == \"happiness\"].sample(frac=0.2).index,\"dataset\"] = \"validation\"\n",
    "df.loc[df[df[\"emotion\"] == \"disgust\"].sample(frac=0.2).index,\"dataset\"] = \"validation\"\n",
    "df.loc[df[df[\"emotion\"] == \"surprise\"].sample(frac=0.2).index,\"dataset\"] = \"validation\"\n",
    "df.loc[df[df[\"emotion\"] == \"fear\"].sample(frac=0.2).index,\"dataset\"] = \"validation\"\n",
    "df.loc[df[df[\"emotion\"] == \"sadness\"].sample(frac=0.2).index,\"dataset\"] = \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4d71df20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train         0.797647\n",
       "validation    0.202353\n",
       "Name: dataset, dtype: float64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"dataset\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3b994c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(\"data\", \"train_validation.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca6656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"train_validation.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca9c91",
   "metadata": {},
   "source": [
    "# Saving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7ac5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"dataset\"] == \"train\"].copy()\n",
    "validation_df = df[df[\"dataset\"] == \"validation\"].copy()\n",
    "\n",
    "#train_df.to_csv(os.path.join(\"data\", \"train.csv\"),index=False)\n",
    "#validation_df.to_csv(os.path.join(\"data\", \"validation.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad207465",
   "metadata": {},
   "source": [
    "# Creating the Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd99e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "validation_df = pd.read_csv(os.path.join(\"data\", \"validation.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1cc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_processor(classes):\n",
    "    return layers.StringLookup(num_oov_indices=0, vocabulary=classes)\n",
    "\n",
    "label_processor = create_label_processor(np.unique(df[\"emotion\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8f97a",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc154267",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df[\"video_path\"].apply(lambda x: list(preprocess_video(x)[0])).values\n",
    "x = [x for x in x]\n",
    "x = np.array(x)\n",
    "x = x.astype(float)\n",
    "\n",
    "y = tf.convert_to_tensor(train_df[\"emotion\"].apply(lambda x: label_processor(x).numpy()).values)\n",
    "y = y[..., None]\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "t_df = Dataset.from_tensor_slices((x, y))\n",
    "t_df.save(os.path.join(\"data\", \"prepared_train_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ae9c3",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = validation_df[\"video_path\"].apply(lambda x: list(preprocess_video(x)[0])).values\n",
    "x = [x for x in x]\n",
    "x = np.array(x)\n",
    "x = x.astype(float)\n",
    "\n",
    "y = tf.convert_to_tensor(validation_df[\"emotion\"].apply(lambda x: label_processor(x).numpy()).values)\n",
    "y = y[..., None]\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "v_df = Dataset.from_tensor_slices((x, y))\n",
    "v_df.save(os.path.join(\"data\", \"prepared_validation_dataset\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa7ba1a6f3da47899bc0f022c057cc653d48904b74cd98a02e2f3dc78b964904"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
